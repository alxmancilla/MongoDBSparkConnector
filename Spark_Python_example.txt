
========  Open PySpark   (jars required) ============
./bin/pyspark --jars mongo-spark-connector_2.11-2.4.1.jar,mongo-java-driver-3.10.2.jar  \
  --conf "spark.mongodb.input.uri=mongodb+srv://<username>:<password>@clusterspark-la6ky.mongodb.net/test.demo?readPreference=primaryPreferred" \
  --conf "spark.mongodb.output.uri=mongodb+srv://<username>:<password>@clusterspark-la6ky.mongodb.net/test.demo"

===  Create people DataFrame ====
people = spark.createDataFrame([("Bilbo Baggins",  50), ("Gandalf", 1000), ("Thorin", 195), ("Balin", 178), ("Kili", 77), ("Dwalin", 169), ("Oin", 167), ("Gloin", 158), ("Fili", 82), ("Bombur", None)], ["name", "age"])

===  Write DataFrame to MongoDB ====
people.write.format("mongo").mode("append").save()

===  Write DataFrame to MongoDB  with options ====
people.write.format("mongo").mode("append").option("database","people").option("collection", "contacts").save()

===  Show Data ====
people.show()

===  Show Schema ====

people.printSchema()

===  Read data from MongoDB into a DataFrame in Spark ====

df = spark.read.format("mongo").load()

===  Read data from MongoDB into a DataFrame in Spark ====

df = spark.read.format("mongo").option("database", "test").option("collection", "demo").load()

===  Show Schema ====

df.printSchema()

===  Show Number of partitions ====
print("Number of partitions: {}".format(df.rdd.getNumPartitions()))

===  Show Partitioner ====
print("Partitioner: {}".format(df.rdd.partitioner))

===  Show number of records per partition ====
def show(index, iterator): yield "partition: "+str(index)+" values: "+ str(len(list(iterator)))

df.rdd.mapPartitionsWithIndex(show).collect()


